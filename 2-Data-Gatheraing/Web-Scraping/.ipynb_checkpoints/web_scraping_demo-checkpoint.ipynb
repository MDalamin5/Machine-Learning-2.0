{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To solve this error use this header para meater so website understand like this request came from web-browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 2.1: Updated headers\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpage = requests.get(\"https://nsu.app/?page=1\", headers=headers).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(webpage, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h1 class=\"text-5xl md:text-6xl font-bold mb-5 text-gray-600\"><span class=\"text-transparent bg-clip-text bg-gradient-to-r from-indigo-500 to-sky-500\">Advising Helper</span></h1>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(soup.prettify()) ## This command is use to show the complete webpage in html tag format\n",
    "soup.find_all('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Advising Helper'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('h1')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h3 class=\"text-lg mb-5 font-light\">Faculty prediction just got better by reviews, Feel free to leave one and help others ðŸŽ“</h3>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('h3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faculty prediction just got better by reviews, Feel free to leave one and help others ðŸŽ“\n"
     ]
    }
   ],
   "source": [
    "for i in soup.find_all(\"h3\"):\n",
    "    print(i.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Course\n",
      "Section\n",
      "Faculty\n",
      "Time\n",
      "Room\n",
      "Semester\n",
      "Rating\n"
     ]
    }
   ],
   "source": [
    "for i in soup.find_all(\"th\"):\n",
    "    print(i.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(soup.find_all(\"tr\", class_=\"even:bg-gray-50\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_courses = soup.find_all('tbody')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_list = soup.find_all(\"tr\", class_=\"even:bg-gray-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = []\n",
    "for course in course_list:\n",
    "    # print(course.find_all('td', class_=\"border-gray-200\"))\n",
    "    course.find_all('td', class_=\"text-gray-500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Course Name': 'ACT202', 'Section Name': '10', 'Faculty Name': 'MRe', 'Time': 'TBA', 'Room': 'TBA', 'Semester': 'Summer 2024 (242)'}\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML content\n",
    "# html_content = \"\"\"<tr class=\"even:bg-gray-50\">...</tr>\"\"\"  # Use your full HTML snippet here.\n",
    "\n",
    "# soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Find the course row\n",
    "course_row = soup.find(\"tr\", class_=\"even:bg-gray-50\")\n",
    "\n",
    "# Extract the relevant columns (td tags)\n",
    "columns = course_row.find_all(\"td\")\n",
    "\n",
    "# Extracting information\n",
    "course_name = columns[0].text.strip()  # Course name\n",
    "section_name = columns[1].text.strip()  # Section name\n",
    "faculty_name = columns[2].find(\"div\", class_=\"font-bold\").text.strip()  # Faculty name\n",
    "time = columns[3].text.strip()  # Time\n",
    "room = columns[4].text.strip()  # Room\n",
    "semester = columns[5].text.strip()  # Semester\n",
    "\n",
    "# Print extracted information\n",
    "dataset = {\n",
    "    \"Course Name\": course_name,\n",
    "    \"Section Name\": section_name,\n",
    "    \"Faculty Name\": faculty_name,\n",
    "    \"Time\": time,\n",
    "    \"Room\": room,\n",
    "    \"Semester\": semester,\n",
    "}\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 50...\n",
      "Scraping page 100...\n",
      "Scraping page 150...\n",
      "Scraping page 200...\n",
      "Scraping page 250...\n",
      "Scraping page 300...\n",
      "Scraping page 350...\n",
      "Scraping page 400...\n",
      "Scraping page 450...\n",
      "Scraping page 500...\n",
      "Scraping page 550...\n",
      "Scraping page 600...\n",
      "Scraping page 650...\n",
      "Scraping page 700...\n",
      "Scraping page 750...\n",
      "Scraping page 800...\n",
      "Scraping page 850...\n",
      "Scraping page 900...\n",
      "Scraping page 950...\n",
      "Scraping page 1000...\n",
      "Scraping page 1050...\n",
      "Scraping page 1100...\n",
      "Scraping page 1150...\n",
      "Scraping page 1200...\n",
      "Total courses scraped: 61050\n",
      "Data saved to courses_data.csv!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Base URL with pagination\n",
    "base_url = \"https://nsu.app/?page=\"  # Replace with the actual URL\n",
    "\n",
    "# Headers to mimic a browser request (update if needed)\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# List to store all course data\n",
    "all_courses = []\n",
    "\n",
    "# Loop through all pages (200 pages)\n",
    "for page in range(1, 1222):  # Adjust range as needed\n",
    "    if(page % 50 == 0):\n",
    "        print(f\"Scraping page {page}...\")\n",
    "    \n",
    "    try:\n",
    "        # Fetch the page\n",
    "        response = requests.get(base_url + str(page), headers=headers, timeout=10)\n",
    "        response.raise_for_status()  # Raise an error if the request fails\n",
    "\n",
    "        # Parse the page content\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Find all course rows (adjust the selector to match your HTML)\n",
    "        course_rows = soup.find_all(\"tr\", class_=\"even:bg-gray-50\")\n",
    "        \n",
    "        for row in course_rows:\n",
    "            columns = row.find_all(\"td\")\n",
    "            \n",
    "            # Extract course details\n",
    "            course_data = {\n",
    "                \"Course Name\": columns[0].text.strip(),\n",
    "                \"Section Name\": columns[1].text.strip(),\n",
    "                \"Faculty Name\": columns[2].find(\"div\", class_=\"font-bold\").text.strip() if columns[2].find(\"div\", class_=\"font-bold\") else \"N/A\",\n",
    "                \"Time\": columns[3].text.strip(),\n",
    "                \"Room\": columns[4].text.strip(),\n",
    "                \"Semester\": columns[5].text.strip(),\n",
    "            }\n",
    "            # Append course data to the list\n",
    "            all_courses.append(course_data)\n",
    "        \n",
    "        # Sleep to avoid being flagged by the server\n",
    "        # time.sleep(2)\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch page {page}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Print the total number of courses scraped\n",
    "print(f\"Total courses scraped: {len(all_courses)}\")\n",
    "\n",
    "# Save the data to a file (optional)\n",
    "import csv\n",
    "with open(\"courses_data.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=all_courses[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_courses)\n",
    "\n",
    "print(\"Data saved to courses_data.csv!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NSU Offered Course data form 2018-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3547\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = \"https://rds2.northsouth.edu/index.php/common/showofferedcourses\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Locate the table\n",
    "table = soup.find(\"tbody\")\n",
    "\n",
    "# Extract data from all rows\n",
    "rows = table.find_all(\"tr\")\n",
    "data = []\n",
    "\n",
    "for row in rows:\n",
    "    cols = row.find_all(\"td\")\n",
    "    data.append([col.text.strip() for col in cols])\n",
    "\n",
    "# Print extracted data\n",
    "# for item in data:\n",
    "#     print(item)\n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.', 'ACT201', '1', 'ARM', 'RA 01:00 PM - 02:30 PM', 'NAC207', '0']\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to courses_spring_25.csv\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "url = \"https://rds2.northsouth.edu/index.php/common/showofferedcourses\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Locate the table\n",
    "table = soup.find(\"tbody\")\n",
    "\n",
    "# Extract data from all rows\n",
    "rows = table.find_all(\"tr\")\n",
    "data = []\n",
    "\n",
    "for row in rows:\n",
    "    cols = row.find_all(\"td\")\n",
    "    data.append([col.text.strip() for col in cols])\n",
    "\n",
    "# Save data to CSV\n",
    "csv_file = \"courses_spring_25.csv\"\n",
    "with open(csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow([\"SRL\", \"Course\", \"Section\", \"Faculty\", \"Time\", \"Room\", \"Seats Available\"])\n",
    "    # Write the data rows\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"Data saved to {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.', 'ACT201', '1', 'ARM', 'RA 01:00 PM - 02:30 PM', 'NAC207', '0']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
